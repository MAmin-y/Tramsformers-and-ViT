# Robust Transformer and Blood Cell Classification with ViT

## Part 1: Robust Transformer Model with Huber Loss Function

In this section, a robust transformer-based model is developed to address challenges in regression tasks under noisy conditions. The approach leverages a self-attention mechanism along with an autoencoder module to capture long-term dependencies in the data. A key component of this model is the integration of the Huber loss function, which is known for its robustness to outliers compared to traditional mean squared error. This design is inspired by the paper “A Robust Loss Function for Neural Network Regression” (refer to [ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0142061524004502)) that details how Huber loss can improve model stability and performance in the presence of noisy inputs. The implementation also makes use of positional encoding to effectively incorporate sequence information into the transformer framework. Overall, the model is configured to explore the benefits of using robust loss functions in deep learning architectures.

## Part 2: Blood Cell Classification Using Vision Transformer

The second part of the project focuses on the classification of blood cell images using a Vision Transformer (ViT) architecture. This part is based on the paper “White Blood Cell Classification: Convolutional Neural Network (CNN) and Vision Transformer (ViT) under Medical Microscope” (refer to [MDPI](https://www.mdpi.com/1999-4893/16/11/525)). The dataset, consisting of images of various blood cell types, is carefully preprocessed and balanced through data augmentation and normalization techniques to ensure high-quality input for the model. The ViT model is fine-tuned using several strategies: freezing the initial encoder layers, freezing the final encoder layers, and full fine-tuning of the entire network. These strategies are evaluated to determine their impact on model performance, including metrics such as accuracy and loss. In addition to the ViT, a conventional CNN model (specifically, DenseNet-121) is trained to serve as a baseline for comparison. This part of the project draws inspiration from recent research on applying transformer models to computer vision tasks, highlighting the trade-offs between different fine-tuning approaches and the potential of ViT to outperform traditional CNN architectures in image classification tasks.
